it would be

position every large their own

all right everybody welcome to the third um meet up uh got Justin here he's going to be doing his talk here on his uh buzz depiction and then uh that'll be about 45 minutes to set up and then we'll do our breakout track so if you want to um pitch something where you want to kind of do an async kind of offline project with people or even a solo thing uh now's your chance that'll be your chance to kind of pitch what that is everybody and let them know and see if get some interest in it um myself and Mike will give us we'll do you know and Mike will do an update uh uh pitches and then we'll do open forum and discuss anything i've got um a uh topic to bring up there and got the J ML um how-to presentation to also kind of give a recap about that and then um we'll wrap up sound good all right Justin

well we are a smallish group so if you have questions just interrupt me um and I will stop and answer whatever questions I have so my name is Justin and I'm going be doing from fuzz to function um basically this idea of we have this abstract world of humans and how do we make how do we turn that into something that's actually solid and concrete um I live here in town i mainly have open source stuff so there's a bunch of open source projects that uh I've created um Andrew is a partner in crime we have an agency here in town called Ray we do a lot of work with UV stuff like that um we're really like big in the Vue.js space so we're sort of software engineers who write you know code and front ends and back ends and whatnot uh but recently we've obviously gotten really into AI and AI is interesting because we kind of have to face what the popular narrative is which is that there's going to be global destruction um although it's sort of hilarious because it alters between global destruction and is something that's going to help me write my code that's basically like all we ever hear about it either Terminator is coming in or I can use cursor to write my code slightly faster um and there's a surprisingly small amount of practitioners that are actually out there creating products and interesting things with it um we've definitely found that there's not even like very good best practices for how to actually use this to do something um so it's a lot of pie in the sky conceptual stuff right now so before I move on let's just talk about those uh uncomfortable questions will AI replace your job if you're a developer this is according to 2024 Stack Overflow most developers think that it will not replace your job um and a few of them think maybe but yeah for the most part no it's not they say it's not going to replace your job so should you use AI to write your code how many of you are developers any of you software engineers okay so good a good mix a little over half um yes you should definitely use it to write your code if you're not already using it i would definitely get on boards as soon as possible will it replace you yes it'll definitely replace you um will it destroy the world now maybe yeah it's going to destroy the world but do you humans have value we still have value all right so there's a little comfort a little warmth in our heart as the world is melting away at least you'll know that you matter um but we're not going to talk about any of this i'm only putting this here as a precursor because you have to do this in every AI talk this is what I'm really going to talk about how do we create AI first apps so you might be familiar now with the magic wand that is popping up on all these different like SAS products and apps and everything like that everybody wants to put some AI into their pre-existing product um and most of the time it's garbage and I don't I don't really recommend this if you guys have a product or you work for a product company and they're like your boss is like "Hey how do we get some AI in here?" like maybe just sit down and rethink the whole thing like how if AI works uh because it will work uh eventually maybe not in this exact moment it will work what would that actually look like so what do I mean by AI first this is my definition my definition is an app where the primary input mechanism is nondeterministic in other words you're not just pushing buttons you're taking in fuzzy input from the outside and that fuzzy could be a photo it could be a text it could be a book it could be the way somebody smiles whatever that fuzzy input is as the primary input mechanism doesn't mean you don't have buttons and other things like that but that's the primary way that you use the app which is obviously in contrast to let's say Figma or one of these other apps where they're adding in some AI features here and there the primary way you use it is still by dragging boxes on the screen but you have some ability to do some neat things um what would it look like if you know you booted up Figma to do your design and it was had none of that was like why don't you draw an app in sketch first you know or something like that we're actually used to this because um man it's been ages since Siri came out I I want to say actually I don't know 2008 maybe 2009 2010 I don't know somewhere around it's a long time ago 2010 later it's been over it's been over 10 years since this came out and its primary input mechanism was spoken word now it was garbage and it still is garbage ironically um but you're kind of already used to this idea of something being AI first so let's let's dream a little bit whole new world um and what I want to like kind of encourage everybody to think is you could actually start building this stuff today like you could go and build a product that's AI first today with the level of quality that exists you don't have to wait for the singularity or some AI revolution or anything like that there's enough intelligence in these things easily enough intelligence to create some real product value out there in fact I would say if AI stopped advancing today there's at least $10 trillion worth of new products that are going to be built on the back of the existing technology there is a lot here so some just some hair brain ideas what about an offhour triage for medical offices like I have kids we call all the time we're like "Hey my kid is sick in the middle of the night what do I do?" They say "I don't know i'll call you back." Maybe if the doctor wants to right um how about something like a personal stylist you wake up every morning you take a picture of yourself and you got your buddy who's like "Ah that green's not doing it for you." Right something like that how about um a curriculum maybe like a homeschool curriculum or something like that that includes the teacher so it actually is you know it's books and things like that with somebody who will walk you through it and help you learn um you could actually build that contract analysis so many contracts out there nobody ever reads them you just give it to it and say like is this good what am I how am I going to get screwed in this contract um you could finally tune an LLM to be incredibly good at that uh a third party insurance advisor right like you could build an insurance advisor that is not tied to any insurance company that can shop the market one of the world's most boring things but the reason it would be really great for AI is you could have a conversation with it right it could it could talk to you and be like "What are the things that matter to you oh you've got kids oh you've got you know uh like a brand new car that matters or that right it's personalized in a way that we really haven't had in the past the problem is we haven't seen any of this i mean very little there's a lot of hubbub in the AI space like around maybe images or videos um there's a little bit of chat body kind of stuff out there but really we haven't seen that much and and the the reality is that AI just isn't very easy to work with um it's like surprisingly hard actually to work with it and that is surprising when we come from some place like Czech GT so why is that well I've got some of the reasons that and this is where I go into sort of war stories now of working with this some of the reasons that I have found this is is hard to work with there is absolutely no standard approach for how to build an app with AI this does not exist yet right if you're going to build a uh some sort of a web like a website or something like that there's half a dozen tools out there no it's way more than that there's dozens and dozens of tools out there that you can use to create an app

tomorrow this is really profound but there is a complete lack of intuition um we all have a sense of builtup intuition about the way the world works and AI I would say is is it's like a little bit like quantum physics or something it defies our preconceived understanding of how things are just going to work they don't just work that way there is very little tooling that exists um there's tools coming out all the time but like they die within a month so the standards are are sort of eroding and shifting constantly um and the models are stupid or stupid expensive like those are the only two options at at this exact moment so no standard approach what do I mean by that well we haven't even solidified like what an like a UI would look like when you're dealing with an app that's AI first the closest thing we have at this point is something like Chion T where you have you you input some text um but it's unclear whether that's going to be in the future or not we don't know like maybe the voice modality really takes off um maybe it's all vision based or maybe it's some amalgamation of all of those kind of things we don't really know yet um things like can you rely on getting a oneshot response back right if you send if you send it the L an LLM or you know even a vision model a query and it comes back with an answer can you can you rely on that actually being true you have to determine that for yourself and for your own application because it's unclear at this point um do you actually need to build an agent what is an agent do we even know um how about like getting structured data out it's pretty useless to just get text output which is what chatbt gives you this there's a little lie inside of these things which I will hopefully uncover a little bit of um but getting structured data out is surprisingly hard how about a lack of personal intuition so if you're a coder you're familiar with the Fibonacci sequence this is a function which produces the Fibonacci sequence and this is the function that produces the Fibonacci sequence and this is also one okay they all do exactly the same thing they're just written in different ways and if you write any code and even if you don't you might have some personal intuition about which of these is better who thinks this is the best how about this one second we got one vote two votes third some hands it's a little bit of a personal thing i kind of like this middle one because I can read it really easily this one uses some fancy hand stuff even though it's nice and concise whatever we get we have a personal intuition that we build up over time if you're a software engineer but how about how about something like this here is a cookie dough recipe creator for an LLM these are the instructions that you're going to send the LLM written in plain English okay so that's one it's written sort of here in a markdown format here's another in sort of an XML like format here's one that's pure text what does your intuition tell you is the best one

it changed for me markdown used to work better than depend on the context there's no clear That's exactly right it completely depends on the model depending on which model you use one of these is better than the other it's crazy and we don't have any intuition built up for which one of these is actually going to be better to work with um and you you eventually build it up through scar tissue I think but so do you really So it sounds like you're moving towards the idea that structures everything work but the idea has to not be structured that's the idea right that's the the idea would be that the question is why does it matter one is XML and one is Oh that's a great question it uh it matters a lot it matters a lot um I can I can uh get into that a little bit but fundamentally the outputs that we get from AI right now are fuzzy by nature so it's fuzzy in and fuzzy out which means that the utility that it provides for you is actually very low um because it takes another fuzzy reader to understand what that output is so chat GPT gives these miraculous answers to you but the output of chat GPT is purely linguistic form it has no real understanding of what it's saying so like an easy example is if you ask Chatch to give you like a cookie dough recipe and you ask for it in a paragraph form they'll write a great one for you if you ask for it in a JSON document it will give you a bad one it's the same thing why does it matter well it matters because it's modeling the linguistic form of the language um and linguistics are even deeper than English or French or German they have they have patterns and ideas and reasoning behind them that are very deep and complicated and frankly humans don't understand them very well which is why there's a debate about whether or not LLMs are the right solution and everybody was surprised that so much intelligence came out of LLMs because we didn't think that language had that much had that much baked into it turns out that it did but you can't build almost anything with that fuzzy output you can only take fuzzy input fuzzy output which is why chat bots are sort of the only thing we've really seen so we want to get more structured output out of these things on the back side um it's the same reason that like stable diffusion um is not necessarily a great thing for photo editing right it's because you like you say okay I want uh you know some guy lifting weights and it'll make a photo for you you say "Ah I want the barbells to be red uh you're you get a whole new image every single time because there's no structure behind it." Is that sort of an answer at least that's my opinion on this tools um there's very few tools out there you want things like like string parsers um you need to do there's no like great testing tools like there are testing tools but there's not a lot of a lot of good ones yet uh LLM chaining there's a bunch of tools that are starting to pop up now for chaining LLMs together um but there's not there's not actually that many that are available um isolating your state is incredibly important like between so if I'm going to chain two LLMs together the second one that I chain I could I might want to give it no context or some context or pieces of context there's not a lot of good tools for that um routing between models so if I if I need something to use uh Gemini and then uh check or GPT 4.1 or something like that getting those routings to work well uh multipplexing streams so streaming is really important where you're getting the information back from the LLM as it's being produced so you don't sit there and wait for it in completion um but and if you do it in multiple streams at the same time you have to figure out a way to get all of that back to the user in some particular way there's not a lot of great options out there for this um context injection there's now like people are starting to develop some tools for context injection but it's still very primitive um and validators structure validators so if I need some structured content back make sure that the structure comes back the right way these are all tools that are in their infancy they are not well established none of them um really own any part of the market i would guess that most of them will be gone in two years models are stupid or stupid expensive this is crazy uh these are both from uh OpenAI exact same prompt sort by the biggest and uh we get a completely inaccurate answer over here and the correct answer over here what's the difference it's only 16 times the price it's the actual price difference between getting the right answer and the wrong answer and this looks great it matched the linguistic form of what I needed it just was wrong which models do this is 40 mini and that is 40

when you say they're stupid oh sorry expensive stupid expensive what do you mean how you need to do this but is the person to hiring a developer if they want to build a website that's very cheap uh well and that it depends on your application it it could be that that's cheap for you but like let's say cursor cursor is losing money they could not possibly lose money faster than they are losing money if they they they are losing money very fast because the actual cost of those LLM requests are high which like they'll fix this right like they'll start to add in they're they're already doing a lot of like context switching and they're starting to have you pay for the higherend models and things like that you can easily I have friends who do exactly what you're saying which is they they pay a developer um and sorry they stop paying developers and now they use exclusively uh AI models and their bills are 10 $20,000 a month for using these models because if you want to use ones that answer correct it's very expensive so that's a that's a market maturity issue too was stupid expensive you know when it started so competition came in scale etc etc etc horrible financial option yes right companies said "Okay I'm going to grow on AWS we want to reuse not just because it's cheap but because it's better right this this this this way we're willing to pay that we got the budget we got our competics model our product cost model we'll make sure you know all that good stuff." So I mean that's a that's a that's a time market time of market it is and there's and there you're identifying there's a gap in the market right now the one gap in market right now is Are we still over the cas you say i think we're probably still over the past yeah probably yeah um there's also like very little in between these these points there's there's not very many models that fall in between the low end and the high end i'm sure they will get developed over time and I'm sure what we think of as a good model now eventually will be trash and cheap um so this this will all change with time um but yeah sorry I have one more question you're doing a lot of that stuff uh is there a place that you go that is tracking the broad accuracy of these numbers that any of us can go to and say this is the crossover rate of this one versus these i mean same place other people go i like chatbot arena a lot to chatbot arena chatbot arena they'll try they have basically like a chesso score um and of course these models like now are sort of trying to gain it you know so like mom 4 got amazing scores even though in practicality maybe doesn't do as well um but the thing I really like about it is they have a um I'll bump it up here real

quick oh no how about

that wet

so they have a leaderboard here with a price analysis um which is one of my favorite things to take a look at and so here you can see on the x-axis you have the cost per token and on the y ais you have the ELO score so in in a perfect world you want to be up here cheap and good um Gemini 2 Flash scores well on these um in general uh but like you know this is here's Gemma 27 uh B and it's it's not actually this good so you have to take these things with a bit of grain of salt um but directionally it's right right like 03 is up in that top right corner it is exceptionally smarter and exceptionally expensive so you do get um some good information out of this but again it's like it depends on your application right um what I'm most interested in is trying to be a practitioner that takes what's available on the market right now and tries to build something compelling out of it and that's where I think things are pretty stupid expensive it's hard to justify the cost um to a consumer maybe B2B would be a little easier but to a consumer it can often be hard to justify the cost um and and be profitable at the same time all right some solutions so patterns are emerging these are some things that we've done um building a tool I'll show you here in a minute um and some things we found you probably want to use a router what do I mean by that well your user gives you a query this is LLM land it goes into your first prompt where you basically have layer on system prompts and things like that and this router is trying to determine effectively how hard is the thing that they're asking you to do how much intelligence does it require and then it produces some kind um of breakdown coming out of that so okay I'm going to go to the simple model i'm going to go to the medium or I'm going to go to the expensive coming out of that it's going to select a function these functions are commensurate with their difficulty right um or a tool right a function call or a tool call and it will select one of these tool calls it will then actually do something deterministic maybe it saves that that function is then capable of saving something to a database creating a CSV um doing something that you actually have structure behind and then finally at the end of it you sandwich it with an LLM reply um so this would work really well in like like a chatbot kind of context or some sort of feedback loop if you want to make it aic which is the hot topic this year you basically give that reply back to the router again and you run the system over again and that's about it this this this little feedback loop is effectively I mean obviously it's always more complicated than that that is effectively just what an agent is you take the output it becomes the input you take the output becomes the input and then the hardest part of an agent is getting it to stop uh and that's also the risky part because it could cost you everything so you definitely want to use tools that last one just showed tools and here's how I describe tools to people who aren't developers and people who are developers that haven't worked with them has anybody ever run this command Emacs done it i'm not waving into this

so this is a game that ships inside of Emacs it's pretty crazy it's a text game it's been there since like 72 or something um and it and it starts and it's like you are in a forest and there's a rogue going off in the distance and laying on the ground is a hammer and and over here is a cup of water what do you want to do um that's what tools are they're just textual descriptions to the LLM you literally just make it up it doesn't mean there's any function or actual tool there you just make up a tool you say "There's a cup of wine how much do you want to pour into?" And then the LLM response that's all a tool is it's nothing magical this is the actual uh Yeah there is a shovel there there's your tool okay so here's here's an example i've got pour beer pour wine uh these are two different tools and this is actually like what you would send to OpenAI if you're unfamiliar with it um you would say what's the name of my function poor beer why would I do this oh this pours a specified amount of beer so on and so forth um and then the result would be something like this it gives you a value back this is how you need to this is like how you need to interact with with uh with AI models LLN specifically if you want to be able to get deterministic like results back out of them um the problem is that you will quickly take this too far um and this is where sort of the intelligence level of these models begins to really show they are quite bad at this they are really quite bad at it um they will they will use the wrong tools all the time they will be overeager to use the tools without enough information um or they'll be overeager to use the tool or under eager to use the tools and they just like won't pour the damn line whatever you know your problems are they will come up and very quickly show you new ones so um that those are tools you probably want to use them you also probably want to use streaming and one of the reasons is the actual experience of using these tools is pretty bad if you wait for the complete response like even chat GBT if you were to take ChatGBT and remove the streaming capability and wait for the entire thing and it just popped on there at once it's a bad experience it's actually a really bad experience so you really want to be getting these tokens as they're coming even if what you're producing at the end of the day is something like an artifact like a let's say a photo or a CSV file or something like that you at least want to know that the tool calls are happening as they're happening and firing them as fast as possible so streaming is the only real way to do that it's interesting because the web is really underdeveloped on this um there's there's like not that much textual streaming um tooling out there right now so I'm imagining in the next year or two this will really start to explode um so change requests this is a great example of like why you would want to use tools or sorry why you would want to use um streaming with tools as the tools are coming in let's say it's poor wine you want to be able to say to the person oh the wine is being poured before you even know how much right so you can dispatch those uh faster you can update the UI in real time of course and it also allows you to do really neat things with introspection on the information that's coming back so one neat feature of um tools is you can automatically Oh

flash um one of the neat things about readable streams is you can just call t on it at any time and it'll take that one stream of bytes and break it into two streams so I do this um in our software all the time where we have a stream of information coming back maybe it's tool calls or something like that i tee it off one of them can go to a logger for us to introspect the other one goes off to the user um I wrote an open source library sort of open source um recently like not too long ago that basically lets you take these streams as they're coming in and interpret them before they're complete so most of these tools return JSON and you can stream the JSON back the problem with JSON is you need the entire string to be able to use it so this tracks how many bytes in we are how many quotations we are and it knows how to close that so you can get streaming JSON as it's being generated um you might want to use multiplexing uh multiplexing is anybody familiar with that term electrical engineering and and whatnot um use this term quite a bit uh it's a it's a simple concept in AI terms I'll describe it like this you have a server maybe you've made three LM requests on the server that are parallel not serial they're all parallel they're happening the same time and we want to stream that back to the user as fast as possible so you basically have one HTTP stream that's open to them and you take all of the data from your LLMs you throw them into that with some sort of bite marker delineation for which ones belong to which channel and then they pop back out on the other side and now you can pass them around to your clients um I'll give you a quick example of how we did this

once so this is a tool that we wrote um we have a product called formkit which like does formulated stuff and I'll just pick one of these

so here we go here's a database table and it's going to send the SQL for you guys can't see any of this can you all right it'll send the SQL along uh to an LLM in this attachment and you can see what's happening here is it's generating a form and it's generating multiple inputs in parallel and by doing that and we can stream those directly to the user instantly through a single stream instead of having to wait for one by one to serially um get created so that's that's what multiplexing can do for

you all right here's a pattern that I've started to adopt it's um it's start with a stupid model if you have this binary classification between a smart and a stupid model start with the stupid ones so I would recommend 4.1 Mini or Flash 2.0 if you're going to build something with an LLM um and the reason is it's sort of like using an old browser when you're building a website um or an old device so when you're building an app you kind of want to start with the lowest level of intelligence and see if you can get your structure to work well there can I architect this thing to work with something that's not that intelligent because then when I add intelligence at least in my experience 100% of the time it improves the product okay so you want to get it as close to usable with this first and then only add the cost where you need the cost um Gemini 2 Flash is actually incredible and incredibly bad it's so good and so it it will reply and it will do incredible replies and and do incredible things and then just completely screw up um 4.1 is much more reliable and just on average a little bit dumber

so you need to build up your intuition for prompts so the first thing that was really hard for me to understand two years ago when I started working on this was that all limbs are stateless um they didn't seem stateless because Chad GPT knew what I said between message one and message two if you are unfamiliar with this it's sort of uh it's the table stakes for working with these things you have to send the entire conversation to the LLM every single time so in chat GBT when you say hi how are you doing and then it says great how are you what what can I do for you today and you say I'd like you to make a cookie dough recipe it sends the entire conversation back to the LLM every single time the hi how are you the whole all the replies everything goes back every single time they are completely stateless and so you are recreating the state every single time um you generally want to constrain your LLM to a small number of of decisions and then give it as little guidance as possible um I have fallen into this trap so many times where basically I am using an allon to do something it does something I'm not expecting i don't like it i go in I throw a little prompt in there it's like hey don't do that okay now my prompts start growing and growing and growing and growing and growing and pretty soon you have conflicting prompts and all other kinds of problems instead what you want to do is try to restrict the number of decisions to be as small as possible and then if you need to add more decisions have a decision tree almost like a phone tree this guy made a high level decision which went down into a lower level decision which went down into a lower level decision um this is very practical but you need to describe the tools in the system prompt and the tool descriptions so um if if you saw back before where we had that JSON block for describing what a tool is it has a description in the tool that says hey this is when you would want to use my me you also need to tell it how to use it in the description i don't know why this is just the intuition that um I built up now also you always want to put all of the dynamic data at the end if you were to look at the system prompt as the top thing that gets sent to the LLM and then the conversation and then somewhere down here is the user all of this should be static static static static static static until the very bottom and the reason is that most of the providers um with Google being the exception they allow you to cash all of that in fact they do it for you automatically and you pay half the cost so if your system prompt has dynamic data in it you cache and validate immediately it's based on chunks of 124 bytes so you don't ever want to put anything dynamic in your system prompt or anything like that you want to put it at the very

end also you don't want to inject any context in the user prompt uh this was a big mistake that I made as well built multiple apps doing this thing wrong um don't use the user prompt to tell the LLM what to do it will get confused almost every single time and it will start talking back to you at some point maybe this is like a month after you've launched your product it'll just start talking to you as if or talking to your user as if it was the user or you were the user or the system is the user it gets very confused so what we do now is we artificially inject a tool call because the the the chat history can also include tool calls so we artificially inject a tool call at the very end that's like load context and that's how and that's how we get our context in there this this is a really key point um and it's really key because it it it defies our initial intuition that the the chat has to be what the chat looks like to the user it does not you can lie cheat and steal to these LLMs all day long and in fact you probably should so let's say that you have uh like Gemini for example we we found that Gemini gets overreinforced on uh the chat context so if the Gemini assistant replies with an emoji and it's like thumbs up then the next time that you're having the conversation it sees that emoji that it replied with last time it's like "Oh maybe I should use one of those again." And then the next time again and pretty soon you'll find these these like almost harmonic resonances starting to build up inside of the AI you can get around this by artificially injecting a message from the user that says "Hey can you stop sending emojis?" and then artificially inject a response from the assistant that's like "Oh my gosh I'm so sorry i'll never do that again." That that part of conversation never happened but it's incredibly effective at redirecting the LMS so you need to learn to lie to the LM in a lot of different ways to get it to sort of do what you need it to do all right so here's the thing that we're building right now um and I'll I'll show it to you just as a a way of kind of getting you guys um thinking about how to use these LLMs this is a personal fitness coach it is not public yet or anything um we're basically I don't know in like friends and family mode right now so like a few people are using it uh but but not very many and it essentially allows anybody to get like a contact card from a trainer just like you would from a person it has a photo it has a phone number and everything and it's your personal trainer you text it information about yourself just using the messages app it texts you back um it creates workouts for you it sets them up you get to see them it tracks it in the gym it'll it'll you know schedule them on the calendar it'll give you an eyecal link that you can use to go like track it it'll send you a reminder the night before be like "Hey you ready for your workout it's tomorrow morning 8 a.m hey 30 minutes to your workout you gonna be there come on don't don't screw up." um you show up gives you a link to a web app where you actually get to track it and it's all powered by AI and it's AI first so I am going to pull up a local version of this now that that's a little different than what you were talking about with the streaming there right because your SMS and RCS whatever it is that's true that is fully turnbased you don't need streaming for that at all so are you finding that the experiences are um no we have to do some cheats in there so if part of that routing process also can tell us like hey if we're going to modify an exercise we know that that's actually pretty expensive because it takes a high level intelligence and we're going to use a thinking model thinking models are slow so when the router has routed it in that direction we will actually send them a reply and be like "Yeah I'll do that for you give me one second." And that's our solution but interestingly we are still using streaming we use it internally even though we're not showing any streaming replies to the user um at least in the text in the in the app part you actually do get some of that but in the text part uh the SMS part you don't but we use it to make um sort of optimistic tool calls even before the stream is complete so if you say "Hey I want to schedule something for tomorrow and do this other thing." As it streams in and it's like "Okay I have the first thing to do let me dispatch that oh I've got the second thing to do dispatch that." And by doing that we're able to speed up what we're what we're doing and have you found that you're able to see um model side faults or form segregations things like that in the multiplex stream were you able to detect those that's a great question um no not yet not yet but you're working on it no we're not going to work on it um we're not going to work on it uh we we do we put some error correction there to try to assist with things um but we only direct stream like that when we are really really confident about the reliability of that policy so OpenAI in particular will guarantee you compliant JSON they make a 100% guarantee on it as long as you flip some switches um and there's some downsides to doing it like you can't do parallel tool calls when you do that um but if you do it they will do the validation on their end as it's streaming out i don't know how they do like this like the back propagation to get that to work but it's very cool and it works um okay let me I'll just show you this onboarding flow here so I go sign up i get to pick my trainer um let's see i'll do Marcus so here's where I would download my contact card i'd add it to my phone i'm not going to do that here is onboarding it's all pre-filled for me cuz I've done this a thousand

times let's actually

text all

right bear with me

it's my coworker L so he won't mind if I buy

this

test good

looking all right

i want to see one okay well he's not in here i'll start with one of these other friends all right here you can see this is a result of an eval here you can see how it worked um this is basically the man you guys really can't see this very well can you this is the admin tool um that we use internally here to produce this

So the these are the conversations that are going back and forth and then we built up our own tooling around this so like I can click on this message and I can see if I make this really big I can see the actual LLM request that was made down here i can see the actual prompt the context the system message that was sent the history state that was actually sent along to the user or to the to the LLM and then the reply so in this case it came in here into the onboarding router it performed an action called set remaining questions this is a trick that we have for having it continue the conversation is if you don't do this it will preemptively call your tools for you um without saying "Hey you've got five questions left to ask," or something like that then what it would do is say "Hey how much do you weigh 180 lbs great here's your workout." Um we don't want it to do that we wanted to actually have a conversation so this is an interesting hack where we use an expensive model with low context to tell a cheap model how much room how much latitude it has to continue to reply um so I can come down here here's my Okay got it that's my workout okay awesome when do I get started and when I send that here you can see these these things firing off schedule workout router so that's the router that decides what does it need to do it decided it needed to clarify information before it moved on so then it went to the responder so if you remember that diagram I had where we had an entry point and then a responder at the very end that's what's happening it does a bunch of decision- making in the middle and then at the very end it is able to look back on that and it can and this is actually great because it also can detect errors through the process so if an error had happened at one of these other points then my responder is given in this case it's it's going to

Gemini remember stateless so we have to recreate state at the end here you can see um these function calls and things like this that are injected in here these are not actual functions that were run these are fake the LLM has no idea what time it is so one way you can solve that is by putting it in the system prompt or something like that well now you're going to invalidate your cash and it also doesn't know when you have a long running conversation if I say if if the LLM says "Hey are you ready for your workout tomorrow?" And then tomorrow comes and it's going to send me another reminder the reminder will probably be like "Are you still ready for your workout tomorrow?" Even if we've told it what today is because the conversation doesn't ever say that time passed it doesn't really know it even if it's in the in the prompting so what we do is we inject these artificial times in there that we say "Hey load current time." Or if there's a big pause in the conversation we'll inject one that says "Uh conversation pause detected right?" And these are sort of the lies that we tell it um up here here you can see this was the tool call performed right here where it asked it to clarify it said [Music] um it said to get to get you started I need to schedule your workouts what days and times work best um for your first workout core instability okay so that's not actually what we sent to the user that just got passed on to the responder through the conversation history the conversation history now includes this and so when the LLM now needs to send another response it has something to go look at um the if if one of these things had failed in there then we can say that the the function response was some form of error and then the responder that comes at the end is able to say hey for some reason I wasn't able to do that can can you ask me again maybe a different way and it it's able to give these really nice fallbacks when you hit errors that sound extremely humanlike it's really wonderful

uh let's See schedule work on the daytime let's say is is the responder nondeterministic or is it also using an lm yeah so every morning at 6 a.m see what those Okay so here's another thing you see here we had two LLMs fail in a row this happens all the time especially with Gemini gemini replied with text that was in a code format this is infuriating i have long conversations with the Google engineers about this uh they don't understand what's going on either um here it tried to call a tool in the text reply called schedule workout that does not exist there is no such tool it was not given such a tool it just decided to hallucinate it we catch that we fail it then we retry here we know we're retrying and we say "Hey you previously failed you failed to reply with a valid text reply to the user." Um and then we explain like what it did wrong and then we rerun the LM again in this case it failed again for the second time so at this point we are done with Gemini 20 flash you failed me and we are going to revert to 40 mini so we have big trees in the codebase of when this LLM fails this is a good substitute if that fails we might need to increase the model uh intensity and we'll actually go to a more expensive model and we have those things kind of mapped out so they know how to how to move between them all um architecturally how did you figure out that it was a fail there's an evaluator for each response that's a great question um it depends sometimes it's just deterministic we get a 429 we get something like that and we can just know that it failed with Gemini this is such a reliable problem that we're actually able to just detect that explicitly in the code but we have yes we have a bunch of deterministic only evals in there that are able to look at replies um we don't use any we we use eval like LLM based evals but we only use them in post-processing because we need to reply to the user too fast um also an interesting note about this we recently ran a lot of testing on

eval like literally it's a roar shock test they are unable to make a determination about the quality of a response um the only models that are able to do that effectively are the expensive ones i I I have picked my brain to death about this i think it has to do with the fact that the linguistic form that an LLM produces is naturally its own linguistic form and so even if it's bad it thinks it's good we ran into a bit of a problem we started we got access to I think it was Chad GT's reasoning model a month ago and we started running those through the reason the reasoning model seems to be better I think the language model I think if the language like the morph I don't know if this is just in my head kind of thing if the morphology is right it's right I can't find anything wrong with it so I can't say wrong with it looks like yeah right but the reason models seem to be pretty good they are they're much much much better expensive they are really expensive yeah we ran tests on like I mean literally almost every model that is in existence Um and the they all either it was like a pass fail like a complete pass fail where we would give it a spe the same exact scenario where there was like an obviously horrifically bad reply and every single cheap model said it was a perfect reply they were this is a nine out of 10 this is very good um and then right when you cross a threshold which in our case was actually we could do it with four uh the latest versions of 40 kind of cross this threshold and they were able to start making good evals on it um and then the reasoning models were the best like like uh 03 or yeah 03 was excellent very excellent so that is a that is an interesting little warning sign that we found um okay hold on i want to get this thing to actually produce some markup have you tried replicating the reasoning process manually because reasons are just chain of thought they are just chain of thought we do that actually yes we do that on on some complicated things like when you're going to plan a brand new workout in our case a brand new workout we found is too hard for it to oneot so we first pass it to a model to use linguistic form right just what it what it is best at to write out a paragraph long description then we take that and we pass into another one that uh tries to structure it and then we take that and we pass it to another one to try to match it to our predefined data it seems like the the prompt chain we build this solves 99% of problems at least for me like if I have an issue first class 80% success if I use another model to look at the first result and initial problem and then see and produce an improved version my accuracy will be 98% session jump with just two okay that's interesting well it depends on the use case i have for example app that does ancient Greek that that takes four steps to produce the so it does a conversion conversion is the length of that variable like if it if it passes a good eval at one time will it say okay two is enough now or I just found out the minimum the minimum 100% fast task so the task turned out to be four steps for four API calls no for [Music] Um one thing that is available over here you can see the actual cost of these things coming through um so like that message and reply uh was a total of a tenth of a penny so you had to build this tool have you considered you might make more money selling the tool than selling we have we have experience selling to developers and decided it's a bad idea there were two guys that did that a long time ago named

Andre uh okay so let's say I don't

know you got me

what's Thursday

the fact that you can have spelling mistakes is amazing

all right so here you can see we got past the onboarding it actually did work and in this case

So here it decided it was going to schedule multiple workouts and this actually produced all of these different function calls um and with exact like you know the target time the exact time it was going to be scheduled for and in and their local time so on and so forth then we can take that result and we can actually do something with it we put it in a database we actually we can actually create determinist results for them um the fact that those happen then starts dispatching things like uh the exercises to actually get created this is where we use a couple of uh simple chains so the result of this exercise router is actual textual content that describes what's what's the right answer here what's going to happen it takes those and then creates actual structured workouts from it and then it takes those and passes them to another function you get this sort of LLM chain um and at the end you're able to actually like go in and

see here I can see my four workouts that are on the calendar i can go into the one that's coming up

here to make sure we're all reading the showers

does your user have a gender

i don't know is that possible

i don't even think gender is a real story we just score sex

so I mean I kind of get the idea what sort of an act this is what sort of a ratio are you finding that you're having to use between the cheap model and the more expensive model uh do you know do you end up you know only barely using the the cheap and we only barely use the expensive okay yeah um yeah the the actual like if I pull this back up here and we go look here here you can see these were expensive right creating those exercises was a whole penny um but if you look up in the rest of this chat here you can see that they're all just fractions these scheduling stuff is expensive so when you like move things around on the calendar that takes a little bit more intelligence but this entire onboarding process is going to the chief actually the largest cost for this is by far the SMS cost sure yeah by far Yeah um and we are thinking about trying to use more intelligent models in a couple of different places which will increase the cost but yeah but in general it's so I've used this various various surface level one workout uh I was impressed that it asked me what questions you had at home and was able to customize exercises that took advantage of my question is but in the future if I say make them workout longer okay is it going to remember that answer where is that stored and is there sort of a rag element how can you talk about that a little bit about what kind of persistent state how you handle persistent statement absolutely um yes so there is there is there is a lot of context about the user available to it including previous workouts the most important part is you have you have this it's so hard to show on this incredibly tiny screen you have this workout rotation here which is like your days you not actually see this as a user but behind the scenes it knows I'm going to be doing a rotation workout basically it gives you a brief prompt about that then anything that you say to it that is like even remotely interesting it will remember it for you i guess my question is how I just remember that if I were naively just thinking about how I was able to do this I might extract some structured things that I wanted to remember and store those in a database yes are you doing that or from your answer yeah it wasn't clear yes we are we are storing it yeah if I if I tell like hey by the way I broke my arm today

you can see here it's setting memory set user profile details to see that so there's a tool called explicit tracking structured thing tracking right so the hard part there is the router needs to be smart enough to be like oh that sounds like something I should remember but it's really bad at doing that so what you want to do is you want it to like o over over select that choice and then get chained to another model that's like hey I think the user I think we should store this but given all of this other context do you think I should and then you provide a tool called do nothing we have tools called do nothing all over the place and we give it that out because otherwise it will choose to do something so we'll say do nothing and at the end it'll actually uh store that so in this case it decided yeah that was this was important so it called that and then chained here and it decided after doing that this was the prompt that it received user broke the arm today um and then after it received that it decided okay I'm going to actually set a memory and then I'm going to follow up on the broken arm at some point in the future so it's a way to say about the broken arm just say sorry can't sleep no it'll give you workouts it'll it'll give you workouts it's pretty It's pretty crazy does it know how long it takes to recover from a broken arm and start i mean it's guessing it said 613 so in one month it's going to check to see pretty impressive so where is it getting that information i've used several semi enabled workout apps i've never been fully hacked so if you're looking for more beta testers that'd be great um but anyway I'm just curious i mean where how does it know how does it know how many reps you're supposed to do what how much weight you're supposed to be lifting how many sets you're supposed to go in i guess that's information's out there on the internet i'm just We Yes that's one of the reasons we use a really high-end model on the initial exercise creation and those models have a lot they have a lot and we we'll use reasoning models on all of them um and they'll reason about those they'll think about it we do lots of iterations and then in the onboarding we ask everything that we can about you so we'll ask your height your weight often the the onboarding the onboarding is sort of like allowed to take you where you go right but on most of them it'll be like oh okay you work out already great what like what's your one rep max on a bench press uh oh you're a runner how many miles have you run this week right right so it'll take it'll try to understand what you need and then it'll use that to start building it out you know and and sometimes it gets it wrong so I don't know why the chat interface is working but the uh like when you're actually in the app you get a chat interface so you can chat with the trainer like in the gym it'll have all your exercise and workouts there and if it's like long on your first one or two you can just say like "Hey uh you you put my bench press at 125 pounds i usually do 250 uh can you just fix the rest of my exercise based on what you think I'd like you know and it'll just rere your whole thing and then once it has a few of those like once you've done a few workouts now it knows it and so that's part of the rag that we pass in every single time and it knows how

to say

dude it's surprisingly good i I have a sister who's a quadriplegic challenge and it makes exercises work for her full barely move around it'll do like neck neck exercises and you know like like little stretches it'll even be like do you have somebody who can help you do these uh you know stand up and sit down you know things like that we just we just wrote a a prototype underneath this form that does adversarial prompting so if you really want to test it yeah we have we have a little bit of that we do run an eval um we we have this eval that we run try to show you this i mean I'm I'm done so you guys feel free to just while you're looking at that if you're able to talk about the platforming and the rest of the infrastructure i'd be curious where you guys are at what you're using what you using to keep stage and mix it on your side is it you know sort of traditional postgressy stuff or this is there's you're generating a lot of it's state to begin with and there's a lot of data that you're going to want to be able to reuse training data really like where you keep that that kind of stuff i'd be curious like what strategy you use for this thing it is fully cloudflare it's all cloud flare all cloud flare we use all of Cloudflare's uh kind of like crazy edge stuff so because it was quick and easy no um there's some of that but we want to be able to scale the LLM requests in parallel really easily okay and so having a node server do that for us is a is just a real pain and um Python in my opinion is just like not a great option for um actual production hosting uh there's there's not like Cloudflare's actually getting better at it themselves but there's not that many good options for super parallel so So when you say Cloudflare are you using Cloud Front you mean Cloudflare cloudflare cloudflare so Cloudflare has a has a product called Cloudflare workers yep um so we use Cloudflare workers we use their D1 um distributed database we use uh R1 which is their like S3 thing i didn't realize I get that part of the infrastructure for the service

part two different stuff that's right r2 i just caught a podcast on it like it's software engineering daily I think just had a podcast on it like two days ago there's we we use their their thing called durable objects which is this really strange compute primitive it's like a tiny compute primitive that has data like a SQL like data storage option attached to it as well as websocket connections as well as HTTP connections um but it's it's like an instance that lives like you can have infinite instances of this so each conversation thread is an instance of one of these uh uh D1 you see it's not like it's not like uh it doesn't just die at 15 minutes so it's right so are they just little containers that they sort of you can't you probably won't know yeah it's abractive for me for the right part right yeah sort of it's it's a weird API it's not particularly developer friendly to use um I don't I don't know that I would recommend it yeah traditionally hasn't been able to do that kind of stuff no interesting uh they're really going into it hard though now and um the nice thing is like I know this thing will scale that's like the one thing I'm not worried about at all so um yeah this so this is like how we run evals that was like an onboarding eval so we ran three of the same personas uh five five different personas three different times we take their average score we we like score the uh lowest response like of any of them and then this is inversely related the the final score is inversely related to how bad the score was so like if one of these gets a one somewhere in there that like for somebody's onboarding journey a one message coming in could be like catastrophic like they're never going to use a product so we want to very heavily weight those out of the system um so this is sort of how we've been making recent decisions on did we do something good when we added something to the pond or did we do something bad um and then I can store these and then save basically a cold storage copy of it run the test again do a comparison to see if I on average went so you're just using SQL stuff you're not you guys didn't build like a file based you know YAML based file based retrieval system using SQL object store and blobs where you need them and pretty basic

Y so you noted the scores I'm curious about like what you're scoring specifically so the way that this works is this same onboarding flow that we saw over here

are

there cloud stuff there are yes they're not great they exist so this is all um Cloudflare has a has a open source project called mini flare which emulates their entire Cloudflare infrastructure um so you can effectively run it local so this is running exactly what we want in production um so here you can see so this is basically a completely brand new cleaned out thing these are all the conversations that just got generated so you can see there's three of each of the names um and then to answer your question what it's scoring is it's taking each reply this is generate this was obviously generated by the AI in the email is taking each reply to the user and determining on a scale of one to 10 basically how good was that reply like was that directionally what we wanted to be talking about would the user be pissed off by it right there's like some some system prompting around that give me a score of 1 to 10 and then it just goes through and and checks them and this is what I was saying if you try to use a cheap model to do this e it does terrible terrible so you have to use a relatively expensive ensive model to say whether or not they were good or bad so I guess like do you have like a reference that you use to measure the AI against kind of like because like right now it's going to be subjective to the AI right yes yeah it's it's purely it is not objective it's all relative internally relative it's all internally relative yeah um there is no like objective basis it's it's because like each each time it's going to be completely different so it's only relative to itself and its own scoring mechanism do you do you anticipate customer success your customer success team supervising that process to evaluate whether truly satisfied happy customers are scoring well well we're still early on we're just getting manual feedback um although we do run this in a shadow eval mode so it's it is evaluating the conversation and then we are able to say like oh that was a bad message and in general it's pretty it's pretty accurate like a low score definitely is a bad message um the only question is sometimes a a questionable message will slip through and be considered good

so yeah any other questions all right this was a time thank

you want to really thank and appreciate the time uh just sort of moving on the agenda I will also remind myself to thank our sponsor studio um and they do have I noticed some day passes there's a little like business card type thing so there that's on the near the name tags over there if anybody wants to grab one of those and come try the co-working space and time that'll be great um organizationally I'll just announce a couple things in uh I think our June meeting want to double check this we have a speaker lined up that's going to be the speaker will be remote but I think what we're going to try to do is be here in the room so we can kind of interact and discuss and have a floating head on the screen so that's Dylan Sheper some of you may know him uh he's going to he's taken a AI first role in his organization he's going to talk about things that he's done for personal AI automation and then parallel track what he's done at work so um very impressive things that he's doing to kind of push the envelope in and I just I'm looking at my phone on the I think we have that with Dylan plan for yes June 24th which is also a Tuesday and so that there'll be announcements and so forth which just as a as a heads up on that date uh for the July meeting what I what we've sort of penciled in i'd like to do a offer I'd like to see if we can do a a community demo day so I think what we got Andrew I asked you about maybe demoing this that that was as much as we would do in the demo day so you don't have to have a professional product but your own uh you know side project or something you're working at home so James you're talking about the Greek translation uh sort sort of a 10 or 15 minute uh walk through here's what I did and just give an opportunity to talk about behind the scenes what you know what architecture or tools or techniques or products did you use and um I have something that I'll can whip into shape by them and uh maybe James and I and I have one or two other folks in mind that I know have projects if you are potentially interested in doing that there's a sign up the theme for every month's signup sheet over there it says demo day write down your name and uh email and we'll put you on a list and try and and get that compiled together so that's far out as I'm looking right now but that's just sort of some visibility about future future movies okay this phase to me is uh the what we call threeminute pitches or or side tracks in general so we have our um monthly meetings and with with speakers and presentations but we also really want to try and make this a platform where people can interact collab and collaborate so we're encouraging um side activities where might be smaller subgroups that work on on things of interest so one of those I can report on we did finally I finally got the book club organized and we had our first meeting of that today um

and we're reading AI engineering which is a kind of an overview of concepts around the types of things that we just saw put into place um not in that little detail but it's it's here's all the here's all the use cases the techniques and then the tools you're kind of left to catch up with that's one thing we identified today is there wasn't a lot of here's the tool that you would use for this today and part of the reason is when this was written at the first of the year that tool is different now and it's going to be different again every month from now so uh we had a we really focused on chapter one today which is sort of the overview point being there's plenty of time to catch up if you want to if you want to also join us um put a signup sheet in the back we're we're meeting every kind of two weeks so the next meeting will be by Zoom i think we're penciling in June 3rd for that and then the subsequent one will be before the June is June so just a heads up on that if anybody wants to wants to join uh and get in without sidetrack okay so this is our chance for other people who want to organize smaller groups i think Nish did you have something in this segment okay great so he has an idea for Yeah um Yeah so hey everybody uh my name is Nish um so my pitch is um I am super interested in kind of how these things work behind the scenes uh the thing that I've most recently taken an interest in is stable fusion um for people that might not have worked with it before it's the uh image generation thing so if you kind of go to chat GPT or Gemini and you kind of ask it to do the cool studio thing was happening a couple months ago um I'm kind of super interested in that process um so the way I kind of view this would be super I don't know kind of like just light touch i do have a Slack channel where the goal is we're all sort of like studying the stuff async um to have an opportunity there to kind of talk to each other ask questions um hold each other accountable in a way uh to actually kind of like gain progress on that stuff um so yeah so super low-level internals um the goal would be at least my goal is to train them from scratch um and kind of you know learn how they work and see where they go so if anyone's interested be sure to chat

slack yeah uh so yeah I'll start a thread um and probably make a channel and then take it from there my experience has also been people will say they're interested it's like who was that and I don't remember where's her video so I've started a signup sheet and uh sign up over there it says stable diffusion inner workings and that just helps kind of collect anybody who's who's interested for future coordination okay other yes AI music anyone interested in music let me know i've been successfully producing for the last two months made more than 50 tracks it's really amazing what's possible to do now it just became possible to make unrecognizable music in most genres not all so if anyone interested in exploring some I'd love to hear the Spotify yeah so that also sounds like a great possibility for a demo day if you want to just show us some of some of those great okay um you got one and I have one more also yeah so the very first meeting I talked about some of the creative endeavors I've been doing with AI [Music]

and so one of the what I pitched last time was uh to use the available tooling to create a series of reenactments of cyber security events in the form of kind of like a Chernobyl kind of story telling and my characters tell the story and I started a Slack channel it's called White Hat i got two other people who were there uh Cass and Kevin and uh started kind of just going through um using Chetch to just generate some just test screen plays uh the first one is based on a real event a Pakistani bank heist of likeund something million dollars uh So it created something that was you know matt so it needs more iteration but um see yeah started doing just some test images here of the different characters potentially the idea did it throw Angelina Julie in there somewhere no uh so the idea is just to kind of just do a very fast first pass of like a super draft get the workflow down try for like the final Um but I ended up taking a bit of a side quest here because Mother's Day was coming up and my past uh uh projects to do these Mother's Day videos where I've used AI and various displays uh for this year

was a bit bottom here but what me and my daughter did was we I use chat to write the story and then uh used a green screen and um she operated the dolls i took clips of these and then I took the dialogue that she was saying and I had the you can't really see here but I used runway ML to animate their mouths and lip sync their mouths to it um this was based on a previous year I did something similar where Chad TV wrote it and he kind of acted it out but it's on the Seville AI Explorers channel you can see the whole thing there there's also extended version where Izzy my daughter it's a bit more unhinged is this supposed to So now Mother's Day screwed so now Mother's Day is over we're going to go redirect my entrance back to you know creating these reenactments of cyber security events

so okay uh one more pitch and then that reminded one or administrative thing to mention uh I had a pitch last month and I just didn't get it organized there were a couple people interested in uh hands-on figure out what MCP is and does so we're going to build a client that connects somewhere to an MTC server and get some stuff and we're going to build MTP server that will offer some things out not uh it's probably a one session or maybe a two session meeting but if you're interested in doing that with me uh we'll pick a time and probably work at a conference room here or something like that sign a few all right uh last administrative thing and uh while still off opening the floor for any other three-minute pitches for sidet tracks uh if you are not on I think most people here are on signal site but if you're not I know we have a couple of new visitors which is awesome signup sheet in the back and we'll you have to be invited to join so if you give us your email address on the signup sheet we'll uh send that out make sure that you have the way to join us that's where we really want to try and do all the uh consolidated communication for this industry okay any other pitches side tracks ideas thoughts

sid track so you have open form okay so our last agenda item and I know we're going long tonight so you know feel free to filter out if we need to um is now we just have we like to try and have open form anything that's on your mind something interesting you saw curious about want to bounce off other people as sort of the group discussion phase um I'll report just very briefly uh one activity that Paul Otto who couldn't be here tonight and I did was the uh regional library JMRL had a what they call a how-to festival they had a lot of little demonstrations and so he and I demonstrated how to use Chat GT or AI to do useful stuff so it was like I showed a uh a person with her book club to help come up with a history of the uh philosophy question that she wanted to discuss in her book club and came up with sample book club questions and she thought that was great uh there was a um kind of a 12-year-old and so we did some image generation but also hey what are you studying in uh school right now what's the topic well we're talking about we're doing angles in geometry okay show me an angle simulator with some slider bars and I use claw canvas to do that and it was pretty it just whipped up a cool thing where you could say well this is an acute angle and slide the bar around a lot of interesting visualization so I think there was a bunch of conversations with that so that was just sort of a public service um trying to spread the word because I don't think every I think we're the general public there's a wide range of exposure right now to somehow

so did you find in general that people weren't aware of what was capable it varied but definitely uh the idea of using canvas to make because I use that for my own class that I've taken this equation I have no idea you know what all the symbols are but I can understand it if to showing a slider bar for each parameter right and and I've got a lot of value for that so people are they they kind of know how to use chat to ask questions but summarize the document would be a next step that people don't really or transform this content into you know four different modes you know a bullet point or a breezy uh executive summary or you know that was kind of one level um but definitely using one thing I just again because I got a lot of personal benefit using canvas to make little visualizers that's not in public consciousness right now i I have a question as I'm watching the water leak through the uh the door seal there uh because I'm coming in this from a a construction background i I'm not really an AI wizard i'm a developer but it's physical stuff it's real estate and so I when I look at something like that how does AI ultimately get used for physical things like buildings to prevent failures that happen throughout the building so I I talk a little bit about that i've kind of got into that i've got a property down in Tai River Virginia that I'm doing timber framing and I'm doing it with hardwood from the property and the two things that I'm slowly working on which are in front of um are log grading is a problem so if you use round logs or you you cut on my sawmill I cut timbers to logs they have to be graded in order for the building inspector that let you use them to build residential property that's a big problem because there's one person that covers Kentucky Virginia and West Virginia that crazy blogs it's like a sixmon wait 35 or $3,000 to get logs for one category so it's really problem so I'm trying to build an AI that can take photographs or videos vlogs or or timber so I cut the timber and I run a camera up turn it point the camera out trying to feed that into an AI to get it to grade it using the rules that these things are as part of that as I got into that chat was like well since you're going to use these for framing let's build an app that does your framing structure and it's actually been pretty impressive and I've been working with it to try to generate it's actually really good at load calculations really good at that it's good a lot of engineering stuff um it's good at uh span calculations it's actually surprisingly good at things like you have to use a king struck um it can you know very very quickly recalculate all the load it can generate a good diagram of load path it's not good to do framing diagrams right so it's it's so the the point being that and I couldn't find anything i was looking for examples out there that I could take inspiration from not much out there that that was happening with construction i did find a ton of planning tools i found a ton of um property maintenance tools things like that uh take a picture of the grass your property send it in we'll give you you know spit out a plan with links to all the products to buy what put them in and how much water they put on top of that stuff like that for for landscaping maintenance but beyond that there was some property management stuff like for uh renters landlords things like that but from from a construction perspective not much really not much i'm involved in carpentry as well i've tried to get I'm interested in the models you're using because I' I've asked for like you'd think it would be able to read the international building code and look at span charts and tell you okay well you got this size of beam this size joist this is a span span I've had wildly inaccurate results and look it's been a few months so I guess maybe they fixed it but like there's a real opportunity there I mean if I was a savbody going to team up in here so I'm sad as you guys that is a hell of an opportunity well when he was talking about loadbearing we just went through we have to submit our plans to a trust company and they take five weeks to turn around and in order to get a permit you got to have certifications trust company so it's like that just delayed our project by 5 weeks which costs x amount of carry costs yada yada the thing is is that if AI can spit out something that's credible does that still get regulatory approval or do they look at that and say "No this is a computer we need the license." That's going to be engineers i It's got to have engineers and and I don't know if the engineers have a lobby i'm sure they probably do because I know how much I paid engineers it's a lot um and they're not going to want to have any government agency say "Yeah it's okay for an AI to pass a stamp." My hope and I've got a I've got a engineer in Charlottesville that says she'll do this we'll see and this is so what I'm trying to do is get this thing to generate framing plans you know so and and it it can so it is so the good news is it generated wrong framing plans but it generated them in a format to be imported into so they're wrong but they're reportable wrong that's actually good that's progress so if I can get it to generate correct framing plans um she you know she says that you know she'll fix it so it's kind of And then I think that's I think that a lot of these kinds of things that are new that's the model that's going to have to be for a while is 604 if if I can show up with something that's 60% of the way there and you can take the last 40 to put your stamp on it you don't make quite as much money but you can do more projects than you make up for charge a little bit more for my 40% i'm still paying less than I would have paid for the 100% you get to do more projects and make more money that's how I talked her to do that i kind of convinced her that this was the way the world was going to be you know and and she was like "Yeah that's true let's try that." So she's willing to do it but I haven't found a model that could do a correct and it cost me money every time I generate a wrong they don't give you money back for a wrong answer right right so that's it gets expensive trying to get the right answer so It's been a bit of a slo um and just in general I found that accurate structural or any kind of accurate when you need something geometrically accurate AI is not that good at it it's not that good you guys sidracked i'll show up at the meetings see if we can get it to build out a deck that doesn't collapse yeah yeah i'm lean I'm leaning into the temper framing is so easy to do it's so easy to frame there only so many things you can do with it like framework is pretty decent honestly but I'm really I really want to try to get the uh the grading out that'd be really good well we we know the construction side but we don't know any of the AI side of it which is why I'm here i thought I probably learned something so whoever figures that out well so when Justin says there's 10 trillion dollars of value for you to me this plan sounds an awful lot like code generation it just needs a different training set to train right different kind of thing it still has to be correct according to its own rules uh and then maybe that's even passed through a local code analysis local building codes and so forth but at some point you still want that engineer with his license and his

liability insurance yeah i I agree i think the engineers are going to start using tools like this to compete against the other engineer faster so the other thing is non-residential structures that you don't need you know so far structures you don't actually need anything structure you just build whatever you want so if you you can have an AI tool that can generate a safe barn that's you know because people don't build you know very high barns and things like that with lots of stuff because they think they got to go easier to do it like see a lot of these white bars you see a lot of uh steel buildings steel red they call red iron buildings they love to be able to build this knife full is what they want to pay an engineer you know do the plan but for building there's no inspection there's nothing so I think there's an opportunity there building you know farm buildings for the world and those guys are always you build that you can generate a plan for the cost of you know engineer opportunity i mean this all this stuff if anybody's been through any big disruption before knows that the technology is the tool it takes to make people with a problem with a hard problem that has opportunities

you know you got to be back against the gym line

have you heard of Zoo zoo they do uh It's an AI generator for a cat file what's the name of it zoo zu Z right there it's an AI it's literally an AI generator for AI generator yeah i'm sure it can do like house plants but it would be interesting to like it can do anything at this point i mean if it would do an elevation of one if you can do a bump that would be one bump that would be a big

There it is all right thanks i'll take a look yeah so at work uh I'm very interested in understanding what the impact of AI is on the organization it's a huge blind spot uh that companies are adopting you know AI blindly and not thinking about you know what what the impact is on the board u so one of the uh articles and reports that came up was this DORA report which is the U Dora the DevOps research uh association and they survey developers and you know gather metrics from all different organizations and they look for this you know they compile a report and they look of the impact of kind of DevOps tooling at on how many students call when they they compile a report um around these kind of key four metrics which is uh deployment frequency how often are these organizations releasing how long is taking them from like a ticket to being pushed to production uh for lead time change uh change failure rate how many errors end up in production and uh how long does it take to fix those errors so some of the metrics there um and they survey developers to see how the you know experiences are uh from you know personal usage of this uh so in general 90% 98% of the organizations are develop or adopting AI um tool uh 76% are using on a daily basis uh you know trillion trillion dollars investment over the five years from these organizations uh you know individual impact you're seeing um you know developers are claiming a 26% flow so just you know experience of writing code to AI tooling is better uh they're experiencing higher productivity u 2% 2.1% you know developers are claiming job satisfaction but they're seeing that the val valuable work that they're doing is decreased inreasing um across the the survey so it's it's um you know asking sped up but it's not reducing the kind of annoying boring work of these people so it's freedom of complaint on Facebook uh yeah they're claiming you know better quality documentation with using them uh better code quality uh review speed uh code complexity is uh you know decreasing um so it's generating improved workflows but not really delivered outcomes and the delivery paradox that they saw was that throughput um has decreased a bit 1.5% across the survey um delivery stability uh decrease uh more errors available in production uh they attributed this potentially larger change bigger change sets so again increased outputs of developers saying "Yay you know I've been so productive but they're uh you creating margin commits with more potential hard to discover errors that uh don't you know fit the the need for production or you know some other edge case um so yeah it it kind of brings up that that there's a a change in the work how work is being done you know funding in the past always want to yeah developers want to you know have control let's

see there's kind of like you know developers in general don't necessarily trust the shipping code uh to production with it Um at 75% report productivity gains like little little but not all trust um organizational

strategies yeah so there needs to be basically kind of a you know strategy around how to better construct um and train you know your organization on what how to use AI effectively um metrics matter and I started you know actually measure these metrics from um try to understand the impact of AI it's actually having on in particular this case it's you know delivery some of the delivery pipeline but it could be other organization other parts of the organization as well um tracking like how much code is actually generated over time um you know maybe creating better tooling around detecting really big PRs and maybe using AI to suggest breaking that up or how to break it up so we I guess the basically the gist of it is that uh there's been a lot of investment for personal productivity uh developers but not much thought downstream of of how to you know create a more robust higher quality delivery pipeline there's not as much uh investment on that and that's starting to show up in the reports um so yeah basically saying to kind of build a culture around

um sharing things as far as you know transparency of where you're using AI and where you're being successful with it um continuous improvement um just a culture sharing um trying to publish with kids how to use AI effectively uh within the organization build a culture around that instead of just giving people tools and

expect figure it out themselves

so trust feedback and stuff like that so that was kind of I thought it was an interesting report because it's kind of

cautionary management upper level upper level companies are saying use AI tools faster but I expected the 2% improvements to be higher it's it's the average across the lot and I think it's just beginning right just starting to see this just starting to have uh AI workflows that are generally much really easily for you before it's much more

hard at a time

they're they're they're taking software if so some examples are if you if you look at if you look at like a big Google originally about oh yeah like transition from your stuff using your stuff works well because oh data so like you get a lot of like games there but And there are people who like kind of new things where AI isn't very good so the games that you're going to see there will see behind this as well as well these guys have to spend a lot of time trying to figure out like how to make it work and so you end up with like the J curve box where it's going to take for a really long time because under some circumstances we're just going to learn yeah these things are not very strong all the time that I put into it right um so kind of at least I can measure that put out there kind are not that way i'm actually surprised that you said they're using it to refactor existing code right yeah that sounds like it'd be hard i think I'm sure they they detail how like bas the methodology so it's like they found a use case to ground and it seems to be working well enough for them that like they publish it they want it um the metrics that they use to measure productivity very specifically are all very like fields metrics essentially told the devs that like if you guys don't feel like it's like 15% more productive or whatever we're going to take the full day and

so I can produce like hey you know folks uh are we keeping this like this is actually good and everybody's like yeah sure Um so I feel 16% exactly 15.8% here yeah so that's what I'm trying to push for you know one I've got access to all the metrics around you know check GT and co-pilot and trying to yeah make sure that's available to everybody in the company so they can see you know then start thinking about how we can tie that back to some sort of business objective the door the door metrics are kind of a good starting point for at least on engineering side but the call I'm going to put out there other faction companies to start think about what your success metrics look like and as we start ramping up you know we're not I'm just starting you put out content like trying to get people to talk about what they're doing I suspect that's going to cause a lot more usage over time and we'll see how that kind of correlates to some of these things that we can quantify and at least know where we're at right because the other risk too is like you know well you are having 15% you know real 15% productivity boost and all of a sudden it's now 10 times more expensive because licenses stuff is highly subsidized you know what's it now look like we have to take this away because you know it's just too much too expensive or cutting cost because some other factors how is that going toffect the organization I will say just and this is mostly my opinion be very wary of anyone who says they know how to measure AI very specifically um I'm yet to see a metric that I'm fully convinced by because like most of them are like kind of like the Google kind of metrics Um others tend to try to make correlations that are really hard to that are really hard to prove um it's either stuff that is non survival or it's metrics that are kind of like stretch um the door for example they're looking at outcomes but I'm sure everyone here can appreciate the fact that a lot of things happen in a project that kind of come you know come together from some type of kind of right like um it could be man we got like a 50% boost in productivity um after we introduced cursor but we also hired like this guy um who's like really good at his job um where like you know so who's to say what you know kind of what resulted in that outcome um so I I would say like you know just my opinion like approach is very simple um because like especially when you see them in headlines visit push just a little bit um you kind of you realize that like yeah the story is like way more complex and the headline the people were driving

I mean I think maybe side lines of code that was generated versus not generated might yeah to challenge that specifically if I generated code and I and it gives me five I edit four of those lines and delete one who's takes credit

i think at that point you deserve credit for it go ahead yeah and so if the AI generates a line and and that line of code results in a be and we end up talking about that line of code we save time

yeah I can see where you're going with that yeah let's see well I mean certainly getting the metrics out there and visible is the first phase where it goes from there we'll see next certainly I'll report back at that point i do wonder if if some of that 2% isn't where somebody's hitting negative numbers right like a little bit of knowledge is Well it could be the sometimes a terrible thing right like they're junior developer they've put out a ton of code this month and well that would be like yeah the engineering the maturity of the engineering or as a whole right and how they run their shop like they just kind of bunch of different developers and giving them cursor right you're going to be really productive but have a lot of problems in production exactly that's going to skew you good cursor says it knows it can see the problem yeah that's okay they're measured by clients code shift they're

happy if any developers want to make a construction have
